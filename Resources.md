# Resources

Resources from the web

# General/Multi
## Collections of AI resources, including tools, models, implementations 
https://github.com/awe50me/Awesome-AI

https://github.com/steven2358/awesome-generative-ai

https://github.com/yunwei37/prompt-hacker-collections

## Learning resources
https://github.com/amusi/awesome-ai-awesomeness

https://github.com/owainlewis/awesome-artificial-intelligence

https://github.com/visenger/awesome-mlops

Demystifying LLMs and Threats My Journey - CSA (Caleb Sima)
https://www.youtube.com/watch?v=q_gDtOu1_7E

Prompts for educators
https://github.com/microsoft/prompts-for-edu/tree/main

## Prompt Injection Primers and research
https://github.com/jthack/PIPE

https://github.com/Cranot/chatbot-injections-exploits

https://github.com/TakSec/Prompt-Injection-Everywhere

https://arxiv.org/abs/2201.11903

Lakera: The ELIF5 Guide to Prompt Injection: Techniques, Prevention, Methods & Tools
https://www.lakera.ai/blog/guide-to-prompt-injection

Safety and Security Risks
of Generative Artificial
Intelligence to 2025 (UK) 
https://assets.publishing.service.gov.uk/media/653932db80884d0013f71b15/generative-ai-safety-security-risks-2025-annex-b.pdf

## Finance:
https://github.com/georgezouq/awesome-ai-in-finance

# Prompt engineering
https://github.com/unbiarirang/Fixed-Input-Parameterization

https://github.com/dair.ai/Prompt-Engineering-Guide

# Attacks
Big list of ML attacks @csima
https://gist.github.com/csima/b5ea16b682f6117c11debee7c40fa8fc

A proof of concept for using ML model file formats to create malware on AI systems:
https://github.com/Azure/counterfit/wiki/Abusing-ML-model-file-formats-to-create-malware-on-AI-systems:-A-proof-of-concept

A collection of code execution techniques for ML or ML adjacent libraries and a sample attack on a blackbox model using Optuna:
https://github.com/moohax/Charcuterie

Universal and transferable adversarial attacks on aligned language models:
https://arxiv.org/pdf/2307.15043.pf

Evasion attacks against machine learning at test time
https://arxiv.org/pdf/1708.06131.pdf

Extracting training data from LLMs
https://arxiv.org/pdf/2012.07805.pdf

Prompt injection (general)
https://github.com/topics/prompt-injection

"tasks where language models get worse as they become better at language modeling (next word prediction)"
https://github.com/mivanit/inverse-scaling-prompt-injection

Injection with special tokens
https://github.com/sunghun7511/chatgpt-prompt-injection

Stored injection POC
https://github.com/JosephTLucas/stored_prompt_injection/blob/main/stored_prompt_injection.ipynb

Inversion attack: privacy recovery
https://github.com/AI-secure/SecretGen

Beyond Memorization: Violating Privacy via inference with large language models
Robin Staab, Mark Vero, Mislav Balunovic, Martin Vechev
https://arxiv.org/pdf/2310.07298v1.pdf

the Offensive ML Framework
https://wiki.offsecml.com/Welcome+to+the+Offensive+ML+Framework

# Research
The Carlini Collection
https://nicholas.carlini.com/writing/2019/all-adversarial-example-papers.html

Exploiting Programmatic Behavior of LLMs: Dual-Use Through Standard Security Attacks
Kang, Li, Stoica, Guestrin, Zaharia, Hashimoto
https://arxiv.org/pdf/2302.05733.pdf
"instruction-following LLMs can produce targeted malicious content, including hate
speech and scams, bypassing in-the-wild defenses
implemented by LLM API vendors."

## Risk Databases / Incidents
https://airisk.io

https://avidml.org

https://incidentdatabase.ai

#Offensive AI
Compilation of resources covering Offensive AI
https://github.com/jiep/offensive-ai-compilation

## CTFs
https://gpa.43z.one

https://gandalf.lakera.ai

https://grt.lakera.ai/mosscap

https://github.com/alexdevassy/Machine_Learning_CTF_Challenges

https://doublespeak.chat

# Build
Building LLM applications for production -  Chip Huyen
https://huyenchip.com/2023/04/11/llm-engineering.html

LLMs, Embeddings, Context Injection, and Next Generation OER
https://opencontent.org/blog/archives/7205

This guy's YouTube has a ton of embeddings/build/use advice

https://www.youtube.com/@RabbitHoleSyndrome

https://www.youtube.com/watch?v=Yhtd7yGGGA 

https://www.youtube.com/watch?v=QdDoFfkVkcw

# Governance
AI Standards
https://aistandardshub.org/ai-standards-search/?_sfm_overall_status=published

NIST crosswalk
https://airc.nist.gov/AI_RMF_Knowledge_Base/Crosswalks

NIST 800-160v1r1 Engineering Trustworthy Secure Systems
https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-160v1r1.pdf

NIST AI RMF
https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf

NIST Taxonomy of AI Risk (Draft)
https://www.nist.gov/system/files/documents/2021/10/15/taxonomy_AI_risks.pdf

CISA Proposed Shared Responsibility Model
https://cloudsecurityalliance.org/blog/2023/07/28/generative-ai-proposed-shared-responsibility-model/

ENISA Multilayer Framework for Good Cybersecurity Practices for AI
https://www.enisa.europa.eu/publications/multilayer-framework-for-good-cybersecurity-practices-for-ai

OWASP LLM Top 10 analysis mapped to other frameworks:
https://github.com/Bobsimonoff/LLM-4-Applications-Commentary/tree/main/docs/LLM-Top-10-Framework-Mappings

# Model Cards
https://modelcards.withgoogle.com/about

https://huggingface.co/blog/model-cards

https://arxiv.org/pdf/1810.03993.pdf

https://github.com/fau-masters-collected-works-cgarbin/model-card-template

https://github.com/ivylee/model-cards-and-datasheets

https://docs.aws.amazon.com/sagemaker/latest/dg/model-cards.html

https://ai.meta.com/blog/system-cards-a-new-resource-for-understanding-how-ai-systems-work/

https://cyclonedx.org/capabilities/mlbom/

# Threat Modeling
Threat Modeling AI Systems: Gavin Klondike

https://aivillage.org/large%20language%20models/threat-modeling-llm/

# M/SDLC
Supply-chain Levels for Software Artifacts (SLSA) 
https://slsa.dev/

Securing the ML Lifecycle, Industry IoT Consortium
https://www.iiconsortium.org/news-pdf/joi-articles/2022-March-JoI-Securing-the-ML-Lifecycle.pdf

Integrating Machine Learning With Software Development Lifecycles: Insights From Experts
Late, Mantymaki, Minkkinen, Birkstedt
https://www.researchgate.net/publication/360318448_Integrating_Machine_Learning_With_Software_Development_Lifecycles_Insights_From_Experts
"This paper examines the challenges related to integrating machine learning (ML) development with software development lifecycle (SDLC) models."

# Defense
https://github.com/Valhall-ai/prompt-injection-mitigations

Blog: a framework to securely use LLMs
https://boringappsec.substack.com/p/edition-22-a-framework-to-securely/


# Tools
Open source vector similarity search for Postgres
https://github.com/pgvector/pgvector

Compare two models side by side
https://sdk.vercel.ai

https://github.com/openai/evals

https://github.com/openai/chatgpt-retrieval-plugin

https://github.com/openai/triton

https://github.com/yoheinakajima/babyagi

https://github.com/Significant-Gravitas/AutoGPT

https://github.com/imartinez/privateGPT

https://github.com/go-skynet/LocalAI

https://github.com/utyvert/mailbuddy

Like JSFiddle, but for prompts:
https://prmpts.ai

## Prompt Injection defense:
https://github.com/protectai/rebuff

https://github.com/derwiki/llm-prompt-injection-filtering

https://github.com/laiyer-ai/llm-guard

https://www.arthur.ai

## Prompt injection target:
https://github.com/svenmorgenrothio/Prompt-Injection-Playground

## Prompt injection harness:
https://github.com/LLMSecurity/HouYi

https://github.com/utkusen/promptmap

## Assessment
A generic automation layer for assessing the security of machine learning systems
https://github.com/Azure/counterfit

"Garak checks if an LLM will fail in any way we don't necessarily want"
https://github.com/leondz/garak

"ART provides tools that enable developers and researchers to defend and evaluate Machine Learning models and applications against the adversarial threats of Evasion, Poisoning, Extraction, and Inference."
https://github.com/Trusted-AI/adversarial-robustness-toolbox

# Prompts
## Non offense
https://huggingface.co/datasets/fka/awesome-chatgpt-prompts

https://huggingface.co/datasets/succinctly/midjourney-prompts

https://huggingface.co/datasets/daspartho/stable-diffusion-prompts

## Negative test
https://huggingface.co/datasets/notrichardren/refuse-to-answer-prompts

## Jailbreak
https://huggingface.co/datasets/rubend18/ChatGPT-Jailbreak-Prompts

https://jailbreakchat.com

http://www.jamessawyer.co.uk/pub/gpt_jb.html

Jailbreaking ChatGPT via Prompt Engineering: An
Empirical Study
Yi Liu, Gelei Deng, Zhengzi Xu, Yuekang Li, Yaowen Zheng, Ying Zhang, Lida Zhao, Tianwei Zhang, Yang Liu
https://arxiv.org/pdf/2305.13860.pdf

## Bias / Fairness
https://huggingface.co/datasets/AlexaAI/bold

https://ai.meta.com/datasets/speech-fairness-dataset/

## Toxicity
https://huggingface.co/datasets/allenai/real-toxicity-prompts

https://huggingface.co/datasets/PanoEvJ/real-toxicity-prompts-severe0.7

## Generate/run code
https://huggingface.co/datasets/LangChainHub-Prompts/LLM_Bash

https://huggingface.co/datasets/agie-ai/awesome-chatgpt-prompts

https://huggingface.co/datasets/kaxap/llama2-sql-instruct-sys-prompt

## Prompt injection
https://huggingface.co/datasets/deepset/prompt-injections

https://huggingface.co/datasets/imoxto/prompt_injection_cleaned_dataset

https://huggingface.co/datasets/imoxto/prompt_injection_cleaned_dataset-v2

https://huggingface.co/datasets/imoxto/prompt_injection_hackaprompt_gpt35

https://huggingface.co/datasets/cgoosen/prompt_injection_password_or_secret

https://huggingface.co/datasets/boardsec/prompt-injection-duplicate-default

https://huggingface.co/datasets/JasperLS/prompt-injections

https://huggingface.co/datasets/jerpint-org/HackAPrompt-Playground-Submissions

https://huggingface.co/datasets/jerpint-org/HackAPrompt-AICrowd-Submissions

Prompt injection with control characters in ChatGPT
https://dropbox.tech/machine-learning/prompt-injection-with-control-characters-openai-chatgpt-llm

## Gandalf and duck
https://github.com/tpai/gandalf-prompt-injection-writeup

https://github.com/Prajwalsrinvas/prompt-injection

# AI-based Attack Tools

## Voice
Real time voice clone and translate
https://www.rask.ai

#AI Transparency
The Foundation Model Transparency Index
https://crfm.stanford.edu/fmti/



